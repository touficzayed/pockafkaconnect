# Prochaines √âtapes - Impl√©mentation du POC

Ce document d√©crit les prochaines √©tapes pour impl√©menter le POC Banking Kafka Connect.

## √âtat Actuel

‚úÖ **Phase 0: Setup et Design (TERMIN√â)**
- Structure du projet cr√©√©e
- Documentation architecture compl√®te
- Configuration templates (local + cloud)
- Docker Compose environment
- Scripts d'automatisation
- Configuration VSCode

---

## Plan d'Impl√©mentation

### Phase 1: Setup de l'Environnement ‚è≥

**Objectif**: Valider que l'infrastructure fonctionne avant de coder.

**Actions:**
1. D√©marrer l'environnement Docker
   ```bash
   cd docker
   docker-compose up -d
   ```

2. G√©n√©rer les cl√©s de test
   ```bash
   ./scripts/generate-test-keys.sh
   ```

3. V√©rifier les services
   - Kafka: http://localhost:9092
   - Kafka Connect: http://localhost:8083
   - MinIO Console: http://localhost:9001

4. Cr√©er un topic de test
   ```bash
   docker exec -it banking-kafka kafka-topics --create \
     --topic payments-in \
     --bootstrap-server localhost:9092 \
     --partitions 3 \
     --replication-factor 1
   ```

**Validation:**
- Tous les services d√©marrent sans erreur
- Kafka Connect REST API r√©pond
- MinIO est accessible et le bucket `banking-payments` existe

---

### Phase 2: SMT HeadersToPayloadTransform üìù

**Objectif**: Extraire les headers Kafka et les ajouter au payload.

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/transforms/
‚îî‚îÄ‚îÄ HeadersToPayloadTransform.java
```

**Tests √† cr√©er:**
```
src/test/java/com/banking/kafka/transforms/
‚îî‚îÄ‚îÄ HeadersToPayloadTransformTest.java
```

**√âtapes d'impl√©mentation:**

1. **Cr√©er la classe de base**
   - √âtendre `org.apache.kafka.connect.transforms.Transformation<SinkRecord>`
   - Impl√©menter les m√©thodes `apply()`, `config()`, `close()`

2. **D√©finir la configuration**
   - ConfigDef avec:
     - `mandatory.headers` (String, required)
     - `optional.headers` (String, default="")
     - `target.field` (String, default="headers")
     - `fail.on.missing.mandatory` (Boolean, default=true)

3. **Impl√©menter la logique**
   - Lire les headers du SinkRecord
   - Valider les headers obligatoires
   - Cr√©er un objet JSON avec les headers
   - Wrapper le payload existant
   - Retourner un nouveau SinkRecord

4. **Tests unitaires**
   - Test avec tous les headers pr√©sents
   - Test avec headers obligatoires manquants
   - Test avec headers optionnels manquants
   - Test avec headers invalides

**Validation:**
```bash
mvn test -Dtest=HeadersToPayloadTransformTest
```

---

### Phase 3: SMT PANTransformationSMT üîê

**Objectif**: G√©rer la transformation du PAN chiffr√© (REMOVE, DECRYPT, REKEY).

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/
‚îú‚îÄ‚îÄ transforms/
‚îÇ   ‚îî‚îÄ‚îÄ PANTransformationSMT.java
‚îî‚îÄ‚îÄ crypto/
    ‚îú‚îÄ‚îÄ JWEHandler.java
    ‚îú‚îÄ‚îÄ KeyStorageProvider.java
    ‚îú‚îÄ‚îÄ FileKeyStorageProvider.java
    ‚îî‚îÄ‚îÄ IBMKeyProtectProvider.java (Phase 7)
```

**Tests √† cr√©er:**
```
src/test/java/com/banking/kafka/
‚îú‚îÄ‚îÄ transforms/
‚îÇ   ‚îî‚îÄ‚îÄ PANTransformationSMTTest.java
‚îî‚îÄ‚îÄ crypto/
    ‚îú‚îÄ‚îÄ JWEHandlerTest.java
    ‚îî‚îÄ‚îÄ KeyStorageProviderTest.java
```

**√âtapes d'impl√©mentation:**

#### 3.1. Mode REMOVE (le plus simple en premier)

1. Cr√©er la classe `PANTransformationSMT`
2. Configurer la strat√©gie `REMOVE`
3. Supprimer le champ `source.field` du payload
4. Tests unitaires pour REMOVE

#### 3.2. JWEHandler (pour DECRYPT et REKEY)

1. Cr√©er `JWEHandler` avec Nimbus JOSE+JWT
   - M√©thode `decrypt(String jwe, RSAPrivateKey key)`
   - M√©thode `encrypt(String plaintext, RSAPublicKey key)`

2. Tests:
   - G√©n√©rer un JWE de test
   - D√©chiffrer avec la cl√© priv√©e
   - Re-chiffrer avec une cl√© publique

#### 3.3. KeyStorageProvider

1. Interface `KeyStorageProvider`
   ```java
   interface KeyStorageProvider {
       RSAPrivateKey getPrivateKey(String keyId);
       RSAPublicKey getPublicKey(String keyId);
   }
   ```

2. Impl√©mentation `FileKeyStorageProvider`
   - Charger les cl√©s depuis le filesystem
   - Caching des cl√©s en m√©moire

3. Tests avec des cl√©s de test

#### 3.4. Mode DECRYPT

1. Int√©grer JWEHandler dans PANTransformationSMT
2. D√©chiffrer le PAN
3. Remplacer dans le target field
4. Tests E2E avec JWE r√©el

#### 3.5. Mode REKEY

1. Charger le mapping des cl√©s partenaires
2. D√©chiffrer avec notre cl√© priv√©e
3. Re-chiffrer avec la cl√© publique du partenaire
4. Tests avec plusieurs institutions

**Validation:**
```bash
mvn test -Dtest=PANTransformationSMTTest
```

---

### Phase 4: Custom Partitioner üìÇ

**Objectif**: Partitioning hi√©rarchique institution/event-type/version/date.

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/partitioner/
‚îî‚îÄ‚îÄ BankingHierarchicalPartitioner.java
```

**Tests √† cr√©er:**
```
src/test/java/com/banking/kafka/partitioner/
‚îî‚îÄ‚îÄ BankingHierarchicalPartitionerTest.java
```

**√âtapes d'impl√©mentation:**

1. √âtendre `io.confluent.connect.storage.partitioner.Partitioner`
2. Lire les headers configur√©s (institution, event-type, version)
3. Construire le chemin: `{institution}/{event-type}/{version}/year={YYYY}/month={MM}/day={DD}/hour={HH}/`
4. G√©rer les valeurs par d√©faut si headers manquants
5. Tests avec diff√©rentes combinaisons de headers

**Validation:**
```bash
mvn test -Dtest=BankingHierarchicalPartitionerTest
```

---

### Phase 5: PGP Encryption (Optionnel) üîí

**Objectif**: Chiffrer les fichiers en streaming avec PGP.

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/crypto/
‚îú‚îÄ‚îÄ PGPEncryptionWrapper.java
‚îî‚îÄ‚îÄ PGPStreamingOutputStream.java
```

**√âtapes d'impl√©mentation:**

1. Wrapper autour du S3 OutputStream
2. Utiliser BouncyCastle pour PGP
3. Streaming encryption (pas de buffering du fichier entier)
4. Tests avec cl√©s PGP de test

**Validation:**
- G√©n√©rer un fichier chiffr√©
- V√©rifier qu'il est d√©chiffrable avec GPG

---

### Phase 6: Tests E2E üß™

**Objectif**: Tester le flow complet avec l'environnement Docker.

**Actions:**

1. **Builder le connector**
   ```bash
   mvn clean package
   cp target/kafka-connect-banking-poc-1.0.0-SNAPSHOT-uber.jar docker/connectors/
   ```

2. **Red√©marrer Kafka Connect**
   ```bash
   docker-compose restart kafka-connect
   ```

3. **D√©ployer le connector**
   ```bash
   curl -X POST http://localhost:8083/connectors \
     -H "Content-Type: application/json" \
     -d @config/local/connector.json
   ```

4. **Cr√©er un producer de test**
   - Script Python/Java qui g√©n√®re des messages
   - Avec headers Kafka appropri√©s
   - Avec PAN chiffr√© en JWE

5. **V√©rifier les fichiers dans MinIO**
   - Ouvrir http://localhost:9001
   - Naviguer dans `banking-payments`
   - V√©rifier la structure de partitioning
   - T√©l√©charger un fichier et valider le format JSONL

6. **Tests de charge**
   - Envoyer 10,000 messages
   - V√©rifier la rotation des fichiers
   - V√©rifier les m√©triques

**Sc√©narios de test:**

| Sc√©nario | Institution | Event Type | Strategy | Attendu |
|----------|-------------|------------|----------|---------|
| 1 | BNK001 | PAYMENT | REMOVE | PAN supprim√© |
| 2 | BNK001 | PAYMENT | DECRYPT | PAN en clair |
| 3 | BNK002 | REFUND | REKEY | PAN re-chiffr√© avec cl√© BNK002 |
| 4 | UNKNOWN | PAYMENT | REMOVE | Utiliser defaults |
| 5 | BNK001 | (manquant) | - | ‚Üí DLQ |

---

### Phase 7: Cloud Deployment (IBM) ‚òÅÔ∏è

**Objectif**: D√©ployer sur IBM Cloud avec Event Streams + COS + Key Protect.

**Pr√©requis:**
- Compte IBM Cloud
- IBM Event Streams instance
- IBM Cloud Object Storage instance
- IBM Key Protect instance

**Actions:**

1. **Cr√©er les ressources IBM Cloud**
   ```bash
   # Event Streams
   ibmcloud resource service-instance-create banking-event-streams \
     messagehub standard us-south

   # COS
   ibmcloud resource service-instance-create banking-cos \
     cloud-object-storage standard global

   # Key Protect
   ibmcloud resource service-instance-create banking-key-protect \
     kms tiered-pricing us-south
   ```

2. **Configurer Key Protect**
   - Importer les cl√©s RSA
   - Cr√©er les policies IAM
   - Tester l'API

3. **Impl√©menter IBMKeyProtectProvider**
   ```java
   src/main/java/com/banking/kafka/crypto/
   ‚îî‚îÄ‚îÄ IBMKeyProtectProvider.java
   ```

4. **Builder l'image Docker**
   ```dockerfile
   FROM confluentinc/cp-kafka-connect:7.6.0
   COPY target/*.jar /usr/share/java/kafka-connect-banking/
   ```

5. **D√©ployer sur Kubernetes (IKS)**
   - Cr√©er les ConfigMaps
   - Cr√©er les Secrets
   - D√©ployer via Helm ou kubectl

6. **Configurer le monitoring**
   - IBM Cloud Monitoring (Sysdig)
   - LogDNA pour les logs
   - Alertes sur les erreurs

---

## Checklist Globale

### Phase 1: Setup ‚è≥
- [ ] Docker Compose up
- [ ] G√©n√©rer les cl√©s de test
- [ ] V√©rifier tous les services
- [ ] Cr√©er le topic Kafka

### Phase 2: HeadersToPayloadTransform
- [ ] Impl√©menter la classe
- [ ] Ajouter tests unitaires
- [ ] Valider avec Maven

### Phase 3: PANTransformationSMT
- [ ] Mode REMOVE
- [ ] JWEHandler
- [ ] KeyStorageProvider (FILE)
- [ ] Mode DECRYPT
- [ ] Mode REKEY
- [ ] Tests E2E

### Phase 4: Partitioner
- [ ] Impl√©menter BankingHierarchicalPartitioner
- [ ] Tests unitaires
- [ ] Valider les chemins g√©n√©r√©s

### Phase 5: PGP (Optionnel)
- [ ] PGPEncryptionWrapper
- [ ] Tests avec GPG

### Phase 6: Tests E2E
- [ ] Builder le connector
- [ ] D√©ployer localement
- [ ] Producer de test
- [ ] Validation MinIO
- [ ] Tests de charge

### Phase 7: Cloud
- [ ] Cr√©er les ressources IBM
- [ ] IBMKeyProtectProvider
- [ ] Docker image
- [ ] D√©ploiement Kubernetes
- [ ] Monitoring

---

## Commandes Utiles

### Maven
```bash
# Compiler
mvn compile

# Tests unitaires
mvn test

# Package
mvn clean package

# Tests d'int√©gration
mvn verify -P integration-tests
```

### Docker
```bash
# D√©marrer
./scripts/start-local-env.sh

# Logs
docker-compose -f docker/docker-compose.yml logs -f kafka-connect

# Arr√™ter
docker-compose -f docker/docker-compose.yml down
```

### Kafka
```bash
# Cr√©er topic
docker exec banking-kafka kafka-topics --create \
  --topic payments-in --bootstrap-server localhost:9092 \
  --partitions 3 --replication-factor 1

# Lister topics
docker exec banking-kafka kafka-topics --list \
  --bootstrap-server localhost:9092

# Consommer
docker exec banking-kafka kafka-console-consumer \
  --topic payments-in --bootstrap-server localhost:9092 \
  --from-beginning
```

### Kafka Connect
```bash
# Lister connectors
curl http://localhost:8083/connectors

# Status
curl http://localhost:8083/connectors/banking-s3-sink/status

# Supprimer
curl -X DELETE http://localhost:8083/connectors/banking-s3-sink
```

---

## Questions / D√©cisions √† Prendre

1. **Performance**: Quel throughput cible? (msgs/sec)
2. **S√©curit√©**: Audit des d√©chiffrements de PAN?
3. **Monitoring**: M√©triques custom √† exposer?
4. **Erreurs**: Comportement si la cl√© partenaire n'existe pas?
5. **Cloud**: Quelle r√©gion IBM Cloud?

---

## Pr√™t √† Commencer?

La structure et la documentation sont maintenant compl√®tes. Vous pouvez:

1. **Option A**: Commencer par la Phase 1 (Setup environnement)
   ```bash
   cd kafka-connect-banking-poc
   ./scripts/start-local-env.sh
   ```

2. **Option B**: Commencer par la Phase 2 (Impl√©menter HeadersToPayloadTransform)
   - Je peux vous guider dans l'impl√©mentation Java

3. **Option C**: Approfondir un aspect sp√©cifique
   - Architecture JWE
   - Int√©gration IBM Key Protect
   - Tests E2E

**Quelle phase souhaitez-vous attaquer en premier?**
