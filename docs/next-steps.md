# Prochaines √âtapes - Impl√©mentation du POC

Ce document d√©crit les prochaines √©tapes pour impl√©menter le POC Banking Kafka Connect.

## √âtat Actuel

‚úÖ **Phases 1-7: Impl√©mentation Core + Scaling (TERMIN√â)**

- ‚úÖ **Phase 1**: Setup environnement (Maven, Docker Compose, Kafka, MinIO)
- ‚úÖ **Phase 2**: HeadersToPayloadTransform (7 tests passants)
- ‚úÖ **Phase 3**: PANTransformationSMT (3 tests passants) - REMOVE, DECRYPT, REKEY
- ‚úÖ **Phase 4**: BankingHierarchicalPartitioner (18 tests passants)
- ‚úÖ **Phase 5**: PGP Encryption avec BouncyCastle (17 tests passants)
- ‚úÖ **Phase 6**: Configuration Multi-Banques
  - BankConfigManager pour configuration centralis√©e
  - Configuration JSON par banque (5 banques: BNK001-BNK005)
  - BankPGPEncryptor pour chiffrement PGP par banque
  - MultiBankPaymentProducer pour tests multi-banques
  - Documentation compl√®te (MULTI_BANK_SETUP.md)
- ‚úÖ **Phase 7**: Scaling et Streaming PGP
  - Partitionneur Murmur2 + mapping CSV d√©terministe (remplace String.hashCode())
  - PGPOutputStreamWrapper pour chiffrement streaming (z√©ro buffering m√©moire)
  - Configuration 20 partitions / 20 tasks pour 200+ banques
  - Tests de distribution 200 banques sur 20 partitions

**Total: 45 tests unitaires passants**

**Fonctionnalit√©s cl√©s impl√©ment√©es:**
- Transformation PAN avec strat√©gies par banque (REMOVE/DECRYPT/REKEY/NONE)
- Chiffrement PGP streaming via `PGPOutputStreamWrapper` (z√©ro buffering m√©moire)
- Partitioning d√©terministe (CSV) ou Murmur2 ‚Äî scalable √† 200+ banques
- 20 tasks parall√®les (~10 banques/task)
- Gestion centralis√©e des configurations multi-banques
- Support de 5 sc√©narios bancaires couvrant tous les cas d'usage

---

## Plan d'Impl√©mentation

### Phase 1: Setup de l'Environnement ‚úÖ (TERMIN√â)

**Objectif**: Valider que l'infrastructure fonctionne avant de coder.

**Actions:**
1. D√©marrer l'environnement Docker
   ```bash
   cd docker
   docker-compose up -d
   ```

2. G√©n√©rer les cl√©s de test
   ```bash
   ./scripts/generate-test-keys.sh
   ```

3. V√©rifier les services
   - Kafka: http://localhost:9092
   - Kafka Connect: http://localhost:8083
   - MinIO Console: http://localhost:9001

4. Cr√©er un topic de test
   ```bash
   docker exec -it banking-kafka kafka-topics --create \
     --topic payments-in \
     --bootstrap-server localhost:9092 \
     --partitions 3 \
     --replication-factor 1
   ```

**Validation:**
- Tous les services d√©marrent sans erreur
- Kafka Connect REST API r√©pond
- MinIO est accessible et le bucket `banking-payments` existe

---

### Phase 2: SMT HeadersToPayloadTransform ‚úÖ (TERMIN√â)

**Objectif**: Extraire les headers Kafka et les ajouter au payload.

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/transforms/
‚îî‚îÄ‚îÄ HeadersToPayloadTransform.java
```

**Tests √† cr√©er:**
```
src/test/java/com/banking/kafka/transforms/
‚îî‚îÄ‚îÄ HeadersToPayloadTransformTest.java
```

**√âtapes d'impl√©mentation:**

1. **Cr√©er la classe de base**
   - √âtendre `org.apache.kafka.connect.transforms.Transformation<SinkRecord>`
   - Impl√©menter les m√©thodes `apply()`, `config()`, `close()`

2. **D√©finir la configuration**
   - ConfigDef avec:
     - `mandatory.headers` (String, required)
     - `optional.headers` (String, default="")
     - `target.field` (String, default="headers")
     - `fail.on.missing.mandatory` (Boolean, default=true)

3. **Impl√©menter la logique**
   - Lire les headers du SinkRecord
   - Valider les headers obligatoires
   - Cr√©er un objet JSON avec les headers
   - Wrapper le payload existant
   - Retourner un nouveau SinkRecord

4. **Tests unitaires**
   - Test avec tous les headers pr√©sents
   - Test avec headers obligatoires manquants
   - Test avec headers optionnels manquants
   - Test avec headers invalides

**Validation:**
```bash
mvn test -Dtest=HeadersToPayloadTransformTest
```

---

### Phase 3: SMT PANTransformationSMT ‚úÖ (TERMIN√â)

**Objectif**: G√©rer la transformation du PAN chiffr√© (REMOVE, DECRYPT, REKEY).

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/
‚îú‚îÄ‚îÄ transforms/
‚îÇ   ‚îî‚îÄ‚îÄ PANTransformationSMT.java
‚îî‚îÄ‚îÄ crypto/
    ‚îú‚îÄ‚îÄ JWEHandler.java
    ‚îú‚îÄ‚îÄ KeyStorageProvider.java
    ‚îú‚îÄ‚îÄ FileKeyStorageProvider.java
    ‚îî‚îÄ‚îÄ IBMKeyProtectProvider.java (Phase 7)
```

**Tests √† cr√©er:**
```
src/test/java/com/banking/kafka/
‚îú‚îÄ‚îÄ transforms/
‚îÇ   ‚îî‚îÄ‚îÄ PANTransformationSMTTest.java
‚îî‚îÄ‚îÄ crypto/
    ‚îú‚îÄ‚îÄ JWEHandlerTest.java
    ‚îî‚îÄ‚îÄ KeyStorageProviderTest.java
```

**√âtapes d'impl√©mentation:**

#### 3.1. Mode REMOVE (le plus simple en premier)

1. Cr√©er la classe `PANTransformationSMT`
2. Configurer la strat√©gie `REMOVE`
3. Supprimer le champ `source.field` du payload
4. Tests unitaires pour REMOVE

#### 3.2. JWEHandler (pour DECRYPT et REKEY)

1. Cr√©er `JWEHandler` avec Nimbus JOSE+JWT
   - M√©thode `decrypt(String jwe, RSAPrivateKey key)`
   - M√©thode `encrypt(String plaintext, RSAPublicKey key)`

2. Tests:
   - G√©n√©rer un JWE de test
   - D√©chiffrer avec la cl√© priv√©e
   - Re-chiffrer avec une cl√© publique

#### 3.3. KeyStorageProvider

1. Interface `KeyStorageProvider`
   ```java
   interface KeyStorageProvider {
       RSAPrivateKey getPrivateKey(String keyId);
       RSAPublicKey getPublicKey(String keyId);
   }
   ```

2. Impl√©mentation `FileKeyStorageProvider`
   - Charger les cl√©s depuis le filesystem
   - Caching des cl√©s en m√©moire

3. Tests avec des cl√©s de test

#### 3.4. Mode DECRYPT

1. Int√©grer JWEHandler dans PANTransformationSMT
2. D√©chiffrer le PAN
3. Remplacer dans le target field
4. Tests E2E avec JWE r√©el

#### 3.5. Mode REKEY

1. Charger le mapping des cl√©s partenaires
2. D√©chiffrer avec notre cl√© priv√©e
3. Re-chiffrer avec la cl√© publique du partenaire
4. Tests avec plusieurs institutions

**Validation:**
```bash
mvn test -Dtest=PANTransformationSMTTest
```

---

### Phase 4: Custom Partitioner ‚úÖ (TERMIN√â)

**Objectif**: Partitioning hi√©rarchique institution/event-type/version/date.

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/partitioner/
‚îî‚îÄ‚îÄ BankingHierarchicalPartitioner.java
```

**Tests √† cr√©er:**
```
src/test/java/com/banking/kafka/partitioner/
‚îî‚îÄ‚îÄ BankingHierarchicalPartitionerTest.java
```

**√âtapes d'impl√©mentation:**

1. √âtendre `io.confluent.connect.storage.partitioner.Partitioner`
2. Lire les headers configur√©s (institution, event-type, version)
3. Construire le chemin: `{institution}/{event-type}/{version}/year={YYYY}/month={MM}/day={DD}/hour={HH}/`
4. G√©rer les valeurs par d√©faut si headers manquants
5. Tests avec diff√©rentes combinaisons de headers

**Validation:**
```bash
mvn test -Dtest=BankingHierarchicalPartitionerTest
```

---

### Phase 5: PGP Encryption ‚úÖ (TERMIN√â)

**Objectif**: Chiffrer les fichiers en streaming avec PGP.

**Fichiers √† cr√©er:**
```
src/main/java/com/banking/kafka/crypto/
‚îú‚îÄ‚îÄ PGPEncryptionWrapper.java
‚îî‚îÄ‚îÄ PGPStreamingOutputStream.java
```

**√âtapes d'impl√©mentation:**

1. Wrapper autour du S3 OutputStream
2. Utiliser BouncyCastle pour PGP
3. Streaming encryption (pas de buffering du fichier entier)
4. Tests avec cl√©s PGP de test

**Validation:**
- G√©n√©rer un fichier chiffr√©
- V√©rifier qu'il est d√©chiffrable avec GPG

---

### Phase 6: Configuration Multi-Banques ‚úÖ (TERMIN√â)

**Objectif**: Permettre des configurations diff√©rentes par banque (strat√©gie PAN + PGP).

**Fichiers cr√©√©s:**
```
src/main/java/com/banking/kafka/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ BankConfigManager.java
‚îú‚îÄ‚îÄ crypto/
‚îÇ   ‚îî‚îÄ‚îÄ BankPGPEncryptor.java
‚îî‚îÄ‚îÄ test/java/com/banking/kafka/integration/
    ‚îî‚îÄ‚îÄ MultiBankPaymentProducer.java

config/banks/
‚îî‚îÄ‚îÄ bank-config.json

docs/
‚îî‚îÄ‚îÄ MULTI_BANK_SETUP.md
```

**Impl√©mentation r√©alis√©e:**

1. **BankConfigManager**
   - Charge la configuration JSON centralis√©e
   - Cache les configurations par banque
   - Fournit une configuration par d√©faut en fallback

2. **BankPGPEncryptor**
   - Chiffrement PGP sp√©cifique par banque
   - Cache des cl√©s publiques par banque
   - Support ASCII armor et binaire selon la banque

3. **MultiBankPaymentProducer**
   - Producteur de test pour 5 banques
   - Couvre tous les sc√©narios (REMOVE, DECRYPT, REKEY, NONE, DECRYPT+Token)
   - Peut tester toutes les banques ou une banque sp√©cifique

**5 Sc√©narios bancaires impl√©ment√©s:**

| Banque | Strat√©gie PAN | PGP | Format PGP | Use Case |
|--------|---------------|-----|------------|----------|
| BNK001 | REMOVE | ‚úÖ | ASCII | Conformit√© stricte PCI-DSS |
| BNK002 | DECRYPT | ‚ùå | - | Syst√®me legacy n√©cessitant PAN clair |
| BNK003 | REKEY | ‚úÖ | Binaire | Isolation avec cl√© propre |
| BNK004 | NONE | ‚úÖ | ASCII | Banque utilisant tokens uniquement |
| BNK005 | DECRYPT+Token | ‚úÖ | ASCII | S√©curit√© maximale (double chiffrement) |

**Validation:**
```bash
# Tester toutes les banques (10 messages par banque)
java -jar target/kafka-connect-banking-poc-*.jar \
  com.banking.kafka.integration.MultiBankPaymentProducer \
  localhost:9092 payments-in 10

# Tester une banque sp√©cifique (50 messages)
java -jar target/kafka-connect-banking-poc-*.jar \
  com.banking.kafka.integration.MultiBankPaymentProducer \
  localhost:9092 payments-in 50 BNK002
```

**Documentation:**
- Guide complet: `MULTI_BANK_SETUP.md`
- Configuration examples pour chaque banque
- V√©rification des r√©sultats dans MinIO/S3
- Tests de charge multi-banques

---

### Phase 7: Tests E2E üß™

**Objectif**: Tester le flow complet avec l'environnement Docker.

**Actions:**

1. **Builder le connector**
   ```bash
   mvn clean package
   cp target/kafka-connect-banking-poc-1.0.0-SNAPSHOT-uber.jar docker/connectors/
   ```

2. **Red√©marrer Kafka Connect**
   ```bash
   docker-compose restart kafka-connect
   ```

3. **D√©ployer le connector**
   ```bash
   curl -X POST http://localhost:8083/connectors \
     -H "Content-Type: application/json" \
     -d @config/local/connector.json
   ```

4. **Cr√©er un producer de test**
   - Script Python/Java qui g√©n√®re des messages
   - Avec headers Kafka appropri√©s
   - Avec PAN chiffr√© en JWE

5. **V√©rifier les fichiers dans MinIO**
   - Ouvrir http://localhost:9001
   - Naviguer dans `banking-payments`
   - V√©rifier la structure de partitioning
   - T√©l√©charger un fichier et valider le format JSONL

6. **Tests de charge**
   - Envoyer 10,000 messages
   - V√©rifier la rotation des fichiers
   - V√©rifier les m√©triques

**Sc√©narios de test:**

| Sc√©nario | Institution | Event Type | Strategy | Attendu |
|----------|-------------|------------|----------|---------|
| 1 | BNK001 | PAYMENT | REMOVE | PAN supprim√© |
| 2 | BNK002 | PAYMENT | DECRYPT | PAN en clair |
| 3 | BNK003 | REFUND | REKEY | PAN re-chiffr√© avec cl√© BNK003 |
| 4 | BNK004 | PAYMENT | NONE | Pas de PAN dans le message |
| 5 | BNK005 | PAYMENT | DECRYPT | PAN tokenis√© |
| 6 | UNKNOWN | PAYMENT | - | Utiliser config default |
| 7 | BNK001 | (manquant) | - | ‚Üí DLQ |

**Tests Multi-Banques:**
```bash
# Envoyer des messages pour toutes les banques
mvn exec:java \
  -Dexec.mainClass="com.banking.kafka.integration.MultiBankPaymentProducer" \
  -Dexec.args="localhost:9092 payments-in 100"

# V√©rifier les fichiers dans MinIO par banque
for bank in bnk001 bnk002 bnk003 bnk004 bnk005; do
  echo "=== $bank ==="
  docker exec banking-minio-init mc find minio/banking-payments/$bank --name "*.json*"
done
```

---

### Phase 8: Cloud Deployment (IBM) ‚òÅÔ∏è ‚è≥ (√Ä VENIR)

**Objectif**: D√©ployer sur IBM Cloud avec Event Streams + COS + Key Protect.

**Pr√©requis:**
- Compte IBM Cloud
- IBM Event Streams instance
- IBM Cloud Object Storage instance
- IBM Key Protect instance

**Actions:**

1. **Cr√©er les ressources IBM Cloud**
   ```bash
   # Event Streams
   ibmcloud resource service-instance-create banking-event-streams \
     messagehub standard us-south

   # COS
   ibmcloud resource service-instance-create banking-cos \
     cloud-object-storage standard global

   # Key Protect
   ibmcloud resource service-instance-create banking-key-protect \
     kms tiered-pricing us-south
   ```

2. **Configurer Key Protect**
   - Importer les cl√©s RSA
   - Cr√©er les policies IAM
   - Tester l'API

3. **Impl√©menter IBMKeyProtectProvider**
   ```java
   src/main/java/com/banking/kafka/crypto/
   ‚îî‚îÄ‚îÄ IBMKeyProtectProvider.java
   ```

4. **Builder l'image Docker**
   ```dockerfile
   FROM confluentinc/cp-kafka-connect:7.6.0
   COPY target/*.jar /usr/share/java/kafka-connect-banking/
   ```

5. **D√©ployer sur Kubernetes (IKS)**
   - Cr√©er les ConfigMaps
   - Cr√©er les Secrets
   - D√©ployer via Helm ou kubectl

6. **Configurer le monitoring**
   - IBM Cloud Monitoring (Sysdig)
   - LogDNA pour les logs
   - Alertes sur les erreurs

---

## Checklist Globale

### Phase 1: Setup ‚úÖ
- [x] Docker Compose up
- [x] G√©n√©rer les cl√©s de test
- [x] V√©rifier tous les services
- [x] Cr√©er le topic Kafka

### Phase 2: HeadersToPayloadTransform ‚úÖ
- [x] Impl√©menter la classe
- [x] Ajouter tests unitaires (15 tests)
- [x] Valider avec Maven

### Phase 3: PANTransformationSMT ‚úÖ
- [x] Mode REMOVE
- [x] JWEHandler
- [x] KeyStorageProvider (FILE)
- [x] Mode DECRYPT
- [x] Mode REKEY
- [x] Tests unitaires (12 tests)

### Phase 4: Partitioner ‚úÖ
- [x] Impl√©menter BankingHierarchicalPartitioner (Murmur2 + CSV mapping)
- [x] Tests unitaires (18 tests, dont distribution 200 banques/20 partitions)
- [x] Valider les chemins g√©n√©r√©s

### Phase 5: PGP ‚úÖ
- [x] PGPEncryptionHandler avec BouncyCastle
- [x] Tests avec g√©n√©ration/d√©chiffrement cl√©s

### Phase 6: Configuration Multi-Banques ‚úÖ
- [x] BankConfigManager (configuration centralis√©e)
- [x] Configuration JSON (5 banques)
- [x] BankPGPEncryptor (chiffrement par banque)
- [x] MultiBankPaymentProducer (tests multi-banques)
- [x] Documentation (MULTI_BANK_SETUP.md)

### Phase 7: Tests E2E ‚è≥
- [x] Builder le connector (uber JAR)
- [x] Producer de test multi-banques
- [ ] D√©ployer localement avec Docker Compose
- [ ] Validation MinIO (fichiers par banque)
- [ ] Tests de charge (1000+ messages)

### Phase 8: Cloud ‚è≥
- [ ] Cr√©er les ressources IBM Cloud
- [ ] IBMKeyProtectProvider
- [ ] Docker image pour Kubernetes
- [ ] D√©ploiement IKS/OpenShift
- [ ] Monitoring (Sysdig, LogDNA)

---

## Commandes Utiles

### Maven
```bash
# Compiler
mvn compile

# Tests unitaires
mvn test

# Package
mvn clean package

# Tests d'int√©gration
mvn verify -P integration-tests
```

### Docker
```bash
# D√©marrer
./scripts/start-local-env.sh

# Logs
docker-compose -f docker/docker-compose.yml logs -f kafka-connect

# Arr√™ter
docker-compose -f docker/docker-compose.yml down
```

### Kafka
```bash
# Cr√©er topic
docker exec banking-kafka kafka-topics --create \
  --topic payments-in --bootstrap-server localhost:9092 \
  --partitions 3 --replication-factor 1

# Lister topics
docker exec banking-kafka kafka-topics --list \
  --bootstrap-server localhost:9092

# Consommer
docker exec banking-kafka kafka-console-consumer \
  --topic payments-in --bootstrap-server localhost:9092 \
  --from-beginning
```

### Kafka Connect
```bash
# Lister connectors
curl http://localhost:8083/connectors

# Status
curl http://localhost:8083/connectors/banking-s3-sink/status

# Supprimer
curl -X DELETE http://localhost:8083/connectors/banking-s3-sink
```

---

## Questions / D√©cisions √† Prendre

1. **Performance**: Quel throughput cible? (msgs/sec)
2. **S√©curit√©**: Audit des d√©chiffrements de PAN?
3. **Monitoring**: M√©triques custom √† exposer?
4. **Erreurs**: Comportement si la cl√© partenaire n'existe pas?
5. **Cloud**: Quelle r√©gion IBM Cloud?

---

## √âtat et Prochaines √âtapes

### ‚úÖ R√©alis√© (Phases 1-6)

**31 tests unitaires passants**

Le POC est fonctionnel avec:
- Transformation PAN avec 4 strat√©gies (REMOVE/DECRYPT/REKEY/NONE)
- Chiffrement PGP optionnel et configurable par banque
- Configuration multi-banques centralis√©e (JSON)
- Partitioning hi√©rarchique
- Producer de test multi-banques

### üöß En Cours (Phase 7: Tests E2E)

**Actions √† r√©aliser:**

1. **D√©ploiement local complet**
   ```bash
   # D√©marrer l'environnement
   cd docker
   docker-compose up -d

   # Copier le connector JAR
   cp target/kafka-connect-banking-poc-*.jar connectors/

   # D√©ployer le connector
   curl -X POST http://localhost:8083/connectors \
     -H "Content-Type: application/json" \
     -d @config/local/connector-multibank.json
   ```

2. **Tests avec producer multi-banques**
   ```bash
   # Envoyer 100 messages pour chaque banque
   java -jar target/kafka-connect-banking-poc-*.jar \
     com.banking.kafka.integration.MultiBankPaymentProducer \
     localhost:9092 payments-in 100
   ```

3. **Validation des fichiers dans MinIO**
   - V√©rifier la structure par banque (bnk001/, bnk002/, etc.)
   - V√©rifier les fichiers PGP (extension .pgp pour banques avec PGP)
   - D√©chiffrer et valider le contenu

4. **Tests de charge**
   - Envoyer 10,000+ messages
   - Mesurer le throughput
   - V√©rifier la rotation des fichiers

### ‚è≥ √Ä Venir (Phase 8: Cloud Deployment)

**D√©ploiement sur IBM Cloud:**
- IBM Event Streams (Kafka manag√©)
- IBM Cloud Object Storage (COS)
- IBM Key Protect pour gestion des cl√©s
- Kubernetes (IKS/OpenShift)
- Monitoring avec Sysdig

### Options Actuelles

**Option A: Compl√©ter les Tests E2E**
```bash
# D√©marrer tout l'environnement local
./scripts/start-local-env.sh
```

**Option B: Ajouter de Nouvelles Banques**
- Modifier `config/banks/bank-config.json`
- Ajouter les cl√©s PGP correspondantes
- Tester avec le producer multi-banques

**Option C: Pr√©parer le Cloud Deployment**
- Impl√©menter `IBMKeyProtectProvider`
- Cr√©er le Dockerfile pour Kubernetes
- Pr√©parer les Helm charts

**Option D: Am√©liorer les Fonctionnalit√©s**
- Ajouter la tokenisation du PAN (BNK005)
- Impl√©menter des m√©triques custom
- Ajouter le support Avro/Parquet

**Quelle option souhaitez-vous poursuivre?**
